<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Teng Hu</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Teng Hu</name>
              </p>
             <p>Hi! I am a Ph.D. student from <strong>Shanghai Jiao Tong University (SJTU)</strong>, under the supervision of <strong><a href="https://yiranran.github.io/">Prof. Ran Yi</a></strong> in Digital Media & Computer Vision Laboratory (<a href="https://dmcv.sjtu.edu.cn/">DMCV</a>).
                My research interests mainly lie in <strong>Computer Vision</strong> and <strong>Generative Models</strong>,
             supported by <strong>CIE-Tencent Doctoral Research Incentive Project (È¶ñÂ±ä‰∏≠ÂõΩÁîµÂ≠êÂ≠¶‰ºö‚ÄîËÖæËÆØÂçöÂ£´ÁîüÁßëÁ†îÊøÄÂä±ËÆ°Âàí(Ê∑∑ÂÖÉÂ§ßÊ®°Âûã‰∏ìÈ°π))</strong>.
             </p>
              <p style="text-align:center">
                <a href="mailto: hu-teng@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.cz/citations?user=Jm5qsAYAAAAJ&hl=zh-CN&authuser=1">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/sjtuplayer">Github</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:60%;max-width:60%;margin-left:90%;text-align:right;">
              <a href="images/me_3.jpg"><img style="width:75%;max-width:75%" alt="profile photo" src="images/me_3.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>My Researches includes </heading>
              <p>
                <p style="text-align:justify">
                    <li style="line-height:125%">
                      <strong>Few-shot Generative Model Adaption</strong>:
                      how to employ generative model in producing high-quality and diverse images in a new domain with only a small number of training data.
                    </li>
                    <li style="line-height:125%">
                      <strong>Stroke-based Neural Painting and Image Vectorization</strong>:
                      how to recreate a pixel-based image with a set of brushstrokes (or scalable vector graphics path) like real human-beings while achieving both faithful reconstruction and stroke style at the same time.
                    </li>
                    <li style="line-height:125%">
                            <strong>Anomaly Generation and Detection</strong>:
                            how to generate anomaly image with few-shot data to help anomaly detection.
                          </li>
              <li style="line-height:125%">
                            <strong>Controllable Image and Video Generation</strong>:
                            how to generate high-quality images and videos with the given conditions. <strong>(main research area)</strong>
              </li>
<!--                    <li style="line-height:125%">-->
<!--                            <strong>Aesthetic Guided Universal Style Transfer</strong>:-->
<!--                            how to transfer the style of an arbitrary image to another content image while striking a balance among aesthetic qualities, style transfromation and content presevation.-->
<!--                          </li>-->
              </p>
              <p>
                <strong>Internship.</strong> We am looking for self-motivated students to join our research group! If you are interested in our research, feel free to contact me!
              </p>
            </td>
          </tr>
        </tbody></table>

<!--        News-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
               <p>
                <strong>[2025.2.27]:</strong> Three papers are accepted by CVPR 2025!
              </p>
               <p>
                <strong>[2025.1.22]:</strong> Our paper <a href="https://arxiv.org/abs/2409.06633">SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation</a> is accepted by ICLR 2025!
              </p>
              <p>
                <strong>[2024.7.24]:</strong> Our paper <a href="https://ieeexplore.ieee.org/document/10607942">FEditNet++: Few-Shot Editing of Latent Semantics in GAN Spaces with Correlated Attribute Disentanglement</a> is accepted by TPAMI 2024!
              </p>
              <p>
                <strong>[2024.7.16]:</strong> Three papers are accepted by ACM Multimedia 2024!
              </p>
              <p>
                <strong>[2024.2.27]:</strong> Our paper <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_SuperSVG_Superpixel-based_Scalable_Vector_Graphics_Synthesis_CVPR_2024_paper.pdf">SuperSVG: Superpixel-based Scalable Vector Graphics Synthesis </a> is accepted by CVPR 2024!
              </p>
              <p>
              <strong>[2023.12.12]:</strong> Our paper <a href = "https://arxiv.org/abs/2311.05276">SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</a> is accepted by ICASSP 2024!
              </p>
              <p>
              <strong>[2023.12.09]:</strong> Our paper <a href = "https://arxiv.org/abs/2312.05767">AnomalyDiffusion: Few-Shot Anomaly Image Generation with Diffusion Model</a> is accepted by AAAI 2024!
              </p>
              <p>
              <strong>[2023.12.05]:</strong> Êàë‰ª¨ÁöÑËÆ∫Êñá <a href=" https://www.sciengine.com/SSI/doi/10.1360/SSI-2023-0194;JSESSIONID=aa676776-2624-49b2-b95a-a02bfef3a2b1">„ÄäÂü∫‰∫éËñÑÊùøÊ†∑Êù°ÊèíÂÄºÁöÑÂºØÊõ≤Á¨îËß¶Á•ûÁªèÁªòÁîª‰∏éÈ£éÊ†ºÂåñÊñπÊ≥ï„Äã</a> Ë¢´‰∏≠ÂõΩÁßëÂ≠¶Ôºö‰ø°ÊÅØÁßëÂ≠¶Êé•Êî∂! Our paper Curve-Stroke-Based Neural Painting and Stylization with Thin Plate Spline Interpolation is accepted by SCIENTIA SINICA Informationis!
              </p>
              <p>
              <strong>[2023.07.26]:</strong> Our paper <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611766">Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region</a> is accepted by ACM MM 2023!
              </p>
              <p>
              <strong>[2023.07.14]:</strong> Our paper <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html">Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</a> is accepted by ICCV 2023!
              </p>

            </td>
          </tr>
        </tbody></table>

<!--        Publications-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Publications (* means equal contribution)</heading>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TPAMI24.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href = "https://ieeexplore.ieee.org/document/10607942"><papertitle>FEditNet++: Few-Shot Editing of Latent Semantics in GAN Spaces with Correlated Attribute Disentanglement</papertitle></a></papertitle>
              </a>
              <br>
              <a href="https://yiranran.github.io/">Ran Yi*</a>,
              <strong>Teng Hu*</strong>,
              <a href="https://thuxmf.github.io/">Mengfei Xia</a>,
              <a href="">Yizhe Tang</a>,
              <a href="https://cg.cs.tsinghua.edu.cn/people/~Yongjin/Yongjin.htm">Yongjin Liu</a>,
              <br>
              <em>Accpeted by <a style="color:red" href="https://ieeexplore.ieee.org/document/10607942"> TPAMI 2024</a> </em>
<!--              <br>-->
<!--               [<a href="https://arxiv.org/pdf/2311.05276.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2311.05276">arXiv</a>]-->
<!--              <br>-->
              <br>
              <a href = "https://ieeexplore.ieee.org/document/10607942">[Paper] </a>
              <br>
              <p></p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/motionmaster.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href = "https://arxiv.org/pdf/2404.15789"><papertitle>MotionMaster: Training-free Camera Motion Transfer For Video Generation</papertitle></a></papertitle>
              </a>
              <br>
              <strong>Teng Hu</strong>,
              <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="">Yating Wang</a>,
              <a href="">Hongrui Huang</a>,
              <a href="">Jieyu Weng</a>,
              <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
              <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
              <br>
              <em>Accpeted by <a style="color:red" href="https://2024.ieeeicassp.org/">ACM MM 2024</a> </em>
<!--              <br>-->
<!--               [<a href="https://arxiv.org/pdf/2311.05276.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2311.05276">arXiv</a>]-->
<!--              <br>-->
              <br>
              <a href = "https://sjtuplayer.github.io/projects/MotionMaster/">[Page] </a><a href = "https://arxiv.org/pdf/2404.15789">[pdf]</a> <a href="https://arxiv.org/abs/2404.15789">[arXiv]</a>
              <br>
              <p></p>
              <p> We propose MotionMaster, a novel training-free video motion transfer model, which disentangles camera motions and object motions in source videos, and transfers the extracted camera motions to new videos.
                We introduce a one-shot and few-shot camera motion disentanglement method, and design a camera motion combination method, enabling our model a more controllable and flexible camera control.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cvpr.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://zwandering.github.io/AUST.github.io/"><papertitle>AesStyler: Aesthetic Guided Universal Style Transfer</papertitle></a>

              <br>
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://zwandering.github.io/"> Haokun Zhu </a>,
              <strong>Teng Hu</strong>,
              <a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&amp;hl=en">Yu-Kun Lai</a>,
              <a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&amp;hl=en">Paul L. Rosin</a>
              <br>
              <em>Accpeted by <a style="color:red" href="https://2024.ieeeicassp.org/">ACM MM 2024</a> </em>
              <br>
              <a href="https://zwandering.github.io/AUST.github.io/">[Page]</a>
              <br>
              <p></p>
              <p>We propose AesStyler, a novel Aesthetic Guided Universal Style Transfer method, which utilizes pre-trained aesthetiic assessment model, a novel Universal Aesthetic Codebook and a novel Universal and Specific Aesthetic-Guided Attention (USAesA) module. Extensive experiments and user-studies have demonstrated that our approach generates aesthetically more harmonious and pleasing results than the state-of-the-art methods.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/supersvg.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href = "https://arxiv.org/abs/2311.05276"><papertitle>SuperSVG: Superpixel-based Scalable Vector Graphics Synthesis</papertitle></a></papertitle>
              </a>
              <br>
              <strong>Teng Hu</strong>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="none">Baihong Qian</a>,
              <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
              <a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Paul L. Rosin</a>
              <a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Yu-Kun Lai</a>,
              <br>
              <em>Accpeted by <a style="color:red" href="https://2024.ieeeicassp.org/">CVPR 2024</a> </em>
<!--              <br>-->
<!--               [<a href="https://arxiv.org/pdf/2311.05276.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2311.05276">arXiv</a>]-->
<!--              <br>-->
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Hu_SuperSVG_Superpixel-based_Scalable_Vector_Graphics_Synthesis_CVPR_2024_paper.pdf">[pdf]</a> <a href="https://web3.arxiv.org/abs/2406.09794">[arXiv]</a>
              <br>
              <p></p>
              <p> We propose SuperSVG, a superpixel-based vectorization model that achieves fast
                and high-precision image vectorization. we decompose the input image into superpixels
                to help the model focus on areas with similar colors and textures.
                Then, we propose a two-stage self-training framework, where a coarse-stage model
                is employed to reconstruct the main structure and a refinement-stage model is used
                for enriching the details.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/icassp.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href = "https://arxiv.org/abs/2311.05276"><papertitle>SAMVG: A Multi-stage Image Vectorization Model with the Segment-Anything Model</papertitle></a></papertitle>
              </a>
              <br>
              <a href="https://zwandering.github.io/"> Haokun Zhu </a>,
              <a href="https://tsb0601.github.io/petertongsb/">Juang Ian Chong</a>,
              <strong>Teng Hu</strong>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://scholar.google.com/citations?user=0i-Nzv0AAAAJ&hl=en">Yu-Kun Lai</a>,
              <a href="https://scholar.google.com/citations?user=V5E7JXsAAAAJ&hl=en">Paul L. Rosin</a>
              <br>
              <em>Accpeted by <a style="color:red" href="https://2024.ieeeicassp.org/">ICASSP 2024</a> </em>
              <br>
               [<a href="https://arxiv.org/pdf/2311.05276.pdf">pdf</a>] [<a href="https://arxiv.org/abs/2311.05276">arXiv</a>]
              <br>
              <p></p>
              <p>We propose SAMVG, a multi-stage model to vectorize raster images into SVG (Scalable Vector Graphics). Through a series of extensive experiments, we demonstrate that SAMVG can produce high quality SVGs in any domain while requiring less computation time and complexity compared to previous state-of-the-art methods.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/aaai2024.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2312.05767"><papertitle>AnomalyDiffusion: Few-Shot Anomaly Image Generation with Diffusion Model</papertitle></a>
              </a>
              <br>
              <strong>Teng Hu</strong>,
              <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://github.com/YuzhenD">Yuzhen Du</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=1621dVIAAAAJ">Xu Chen</a>,
              <a href="https://orcid.org/0000-0001-7910-810X/">Liang Liu</a>,
              <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
              <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>
              <br>
              <em>Accepted by <a style="color:red" href="https://aaai.org/aaai-conference/">AAAI 2024</a></em>
              <br>
              [<a href="https://arxiv.org/pdf/2312.05767.pdf">pdf</a>]  [<a href="https://github.com/sjtuplayer/anomalydiffusion">code</a>] [<a href="https://arxiv.org/abs/2312.05767">arXiv</a>]
              <br>
              <p></p>
              <p>we propose AnomalyDiffusion, a novel diffusion-based few-shot anomaly generation model, which utilizes the strong prior information of latent diffusion model learned from large-scale dataset to enhance the generation authenticity under few-shot training data.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/‰∏≠ÂõΩÁßëÂ≠¶.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://www.sciengine.com/SSI/home?slug=abstracts&abbreviated=scp"><papertitle>Curve-Stroke-Based Neural Painting and Stylization with Thin Plate Spline Interpolation</papertitle></a>
              </a>
              <br>
              <a href="">Bohao Tang*</a>,
              <strong>Teng Hu*</strong>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://github.com/YuzhenD">Yuzhen Du</a>,
              <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
              <br>
              <em>Accepted by <a style="color:red" href="https://www.sciengine.com/SSI/home?slug=abstracts&abbreviated=scp">‰∏≠ÂõΩÁßëÂ≠¶Ôºö‰ø°ÊÅØÁßëÂ≠¶ (SCIENTIA SINICA Informationis)</a></em>
              <p></p>
              <p> We propose a new curved brushstroke parameter model based on thin-plate spline interpolation. By curving and affine-transforming real brushstroke templates in succession, we can generate more realistic and varied brushstroke images. Furthermore, we propose a hierarchical brushstroke optimization method that decomposes the entire image into multiple brushstrokes, from large to small, effectively improving the model‚Äôs painting ability for both the overall structure and local details of the image</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mm.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611766"><papertitle>Stroke-based Neural Painting and Stylization with Dynamically Predicted Painting Region</papertitle></a>
              </a>
              <br>
              <strong>Teng Hu</strong>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://zwandering.github.io/"> Haokun Zhu </a>,
              <a href="https://orcid.org/0000-0001-7910-810X/">Liang Liu</a>,
              <a href="https://orcid.org/0009-0003-1887-6406/">Jinlong Peng</a>,
              <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
              <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>
              <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
              <br>
              <em>Accepted by <a style="color:red" href="https://www.acmmm2023.org/">ACM MM 2023</a></em>
              <br>
              [<a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3611766">pdf</a>] [<a href="https://github.com/sjtuplayer/Compositional_Neural_Painter">code</a>] [<a href="https://arxiv.org/abs/2309.03504">arXiv</a>]
              <br>
              <p></p>
              <p>We propose Compositional Neural Painter, a novel stroke-based rendering framework which dynamically predicts the next painting region based on the current canvas, instead of dividing the image plane uniformly into painting regions. Extensive experiments show our model outperforms the existing models in stroke-based neural painting.</p>
            </td>
            </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html"><papertitle>Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</papertitle></a>
              </a>
              <br>
              <strong>Teng Hu</strong>,
              <a href="https://zhangzjn.github.io/">Jiangning Zhang</a>,
              <a href="https://orcid.org/0000-0001-7910-810X/">Liang Liu</a>,
              <a href="https://yiranran.github.io/">Ran Yi</a>,
              <a href="https://github.com/karrykkk">Siqi Kou</a>,
              <a href="https://zwandering.github.io/"> Haokun Zhu </a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=1621dVIAAAAJ">Xu Chen</a>,
              <a href="https://orcid.org/0000-0002-6592-8411/">Yabiao Wang</a>,
              <a href="https://orcid.org/0000-0003-4216-8090/">Chengjie Wang</a>
              <a href="https://orcid.org/0000-0003-1653-4341/">Lizhuang Ma</a>
              <br>
              <em>Accepted by <a style="color:red" href="https://iccv2023.thecvf.com/">ICCV 2023</a></em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.pdf">pdf</a>] [<a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_supplemental.pdf">supp</a>] [<a href="https://github.com/sjtuplayer/few-shot-diffusion">code</a>] [<a href="https://arxiv.org/abs/2309.03729">arXiv</a>]
              <br>
              <p></p>
              <p>We propose a novel phasic content fusing few-shot diffusion model with directional distribution consistency loss, which targets different learning objectives at distinct training stages of the diffusion model. Theoretical analysis, and experiments demonstrate the superiority of our approach in few-shot generative model adaption tasks.</p>
            </td>
          </tr>
      </tbody>
    </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors & Awards</heading>
              <li>
                <strong>CIE-Tencent Doctoral Research Incentive Project (È¶ñÂ±ä‰∏≠ÂõΩÁîµÂ≠êÂ≠¶‰ºö‚ÄîËÖæËÆØÂçöÂ£´ÁîüÁßëÁ†îÊøÄÂä±ËÆ°Âàí(Ê∑∑ÂÖÉÂ§ßÊ®°Âûã‰∏ìÈ°π)), 2024</strong>
              </li>
              <li>
                <strong>Zhiyuan Outstanding Student Scholarship of ShanghaiJiao Tong University, 2022</strong>
              </li>
              <li>
                <strong>Zhiyuan Honors Bachelor's Degree of Shanghai Jiao Tong University, 2022</strong>
              </li>
              <li>
                <strong>Outstanding Graduate of Shanghai Jiao Tong University, 2022</strong>
              </li>
              <li>
                <strong>Zhiyuan Honors Scholarship of Shanghai Jiao Tong University, 2018-2021 </strong>
              </li>
            </td>
          </tr>
        </tbody></table>


  </table>
</body>

</html>
